{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rtdAJALCwJ"
      },
      "source": [
        "# config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzjAsnz1JvOW",
        "outputId": "64c19bb7-8166-472a-af1a-861ca1e5b2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter GitHub token (hidden): ··········\n",
            "200\n",
            "Vu-Quoc-Tuan\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import requests, json\n",
        "\n",
        "token = getpass(\"Enter GitHub token (hidden): \")\n",
        "\n",
        "r = requests.get(\"https://api.github.com/user\", headers={\"Authorization\": f\"token {token}\"})\n",
        "print(r.status_code)\n",
        "print(r.json().get(\"login\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sBwvWH0oKQos",
        "outputId": "7496dfcd-9424-45ea-f1ce-457312a3cb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BTL-NLP-2526I_INT3406_3'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 232 (delta 26), reused 41 (delta 18), pack-reused 177 (from 1)\u001b[K\n",
            "Receiving objects: 100% (232/232), 136.89 MiB | 16.18 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n",
            "Updating files: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "username = \"Vu-Quoc-Tuan\"\n",
        "repo_name = \"BTL-NLP-2526I_INT3406_3\"\n",
        "\n",
        "# Sử dụng biến token để xác thực trực tiếp trong URL\n",
        "!git clone https://{token}@github.com/{username}/{repo_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TN2ebbTWJ0HR",
        "outputId": "619e27d9-55c8-4c1d-ca08-705ebdf992f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BTL-NLP-2526I_INT3406_3\n",
            "/content/BTL-NLP-2526I_INT3406_3\n",
            "total 28\n",
            "drwxr-xr-x 5 root root 4096 Dec 22 11:26 .\n",
            "drwxr-xr-x 1 root root 4096 Dec 22 11:26 ..\n",
            "drwxr-xr-x 8 root root 4096 Dec 22 11:26 .git\n",
            "-rw-r--r-- 1 root root   60 Dec 22 11:26 .gitignore\n",
            "-rw-r--r-- 1 root root   25 Dec 22 11:26 README.md\n",
            "drwxr-xr-x 4 root root 4096 Dec 22 11:26 transformer_base\n",
            "drwxr-xr-x 4 root root 4096 Dec 22 11:26 vlsp-mt\n",
            "total 80\n",
            "drwxr-xr-x 4 root root  4096 Dec 22 11:26 .\n",
            "drwxr-xr-x 5 root root  4096 Dec 22 11:26 ..\n",
            "drwxr-xr-x 6 root root  4096 Dec 22 11:26 data\n",
            "-rw-r--r-- 1 root root 39689 Dec 22 11:26 REPORT.md\n",
            "-rw-r--r-- 1 root root   485 Dec 22 11:26 requirements.txt\n",
            "-rw-r--r-- 1 root root 11658 Dec 22 11:26 RUN_GUIDE.md\n",
            "-rw-r--r-- 1 root root  4621 Dec 22 11:26 run.md\n",
            "drwxr-xr-x 3 root root  4096 Dec 22 11:26 scripts\n",
            "requirements found\n",
            "Python executable: /usr/bin/python3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 2)) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 3)) (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 4)) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 7)) (4.57.3)\n",
            "Requirement already satisfied: accelerate>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 8)) (1.12.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 10)) (5.29.5)\n",
            "Requirement already satisfied: peft>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 13)) (0.18.0)\n",
            "Collecting bitsandbytes>=0.43.0 (from -r vlsp-mt/requirements.txt (line 14))\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: datasets>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 17)) (4.0.0)\n",
            "Collecting sacrebleu (from -r vlsp-mt/requirements.txt (line 20))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiwer (from -r vlsp-mt/requirements.txt (line 21))\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 24)) (5.2.0)\n",
            "Collecting datasketch (from -r vlsp-mt/requirements.txt (line 27))\n",
            "  Downloading datasketch-1.8.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 30)) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 31)) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 32)) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 33)) (1.6.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 36)) (0.23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r vlsp-mt/requirements.txt (line 39)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r vlsp-mt/requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r vlsp-mt/requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (0.70.16)\n",
            "Collecting portalocker (from sacrebleu->-r vlsp-mt/requirements.txt (line 20))\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r vlsp-mt/requirements.txt (line 20)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->-r vlsp-mt/requirements.txt (line 20))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r vlsp-mt/requirements.txt (line 20)) (6.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r vlsp-mt/requirements.txt (line 21)) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer->-r vlsp-mt/requirements.txt (line 21))\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r vlsp-mt/requirements.txt (line 33)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r vlsp-mt/requirements.txt (line 33)) (3.6.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r vlsp-mt/requirements.txt (line 36)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r vlsp-mt/requirements.txt (line 36)) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r vlsp-mt/requirements.txt (line 36)) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r vlsp-mt/requirements.txt (line 36)) (2.47.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r vlsp-mt/requirements.txt (line 36)) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r vlsp-mt/requirements.txt (line 36)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r vlsp-mt/requirements.txt (line 36)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r vlsp-mt/requirements.txt (line 36)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0->-r vlsp-mt/requirements.txt (line 7)) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r vlsp-mt/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r vlsp-mt/requirements.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r vlsp-mt/requirements.txt (line 36)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.18.0->-r vlsp-mt/requirements.txt (line 17)) (1.17.0)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading datasketch-1.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: rapidfuzz, portalocker, colorama, sacrebleu, jiwer, datasketch, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.0 colorama-0.4.6 datasketch-1.8.0 jiwer-4.0.0 portalocker-3.2.0 rapidfuzz-3.14.3 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "# 1) vào thư mục repo (thay path nếu khác)\n",
        "%cd /content/BTL-NLP-2526I_INT3406_3\n",
        "\n",
        "# 2) kiểm tra hiện tại đang ở đâu và liệt kê file\n",
        "!pwd\n",
        "!ls -la\n",
        "!ls -la vlsp-mt\n",
        "!test -f vlsp-mt/requirements.txt && echo \"requirements found\" || echo \"requirements NOT found\"\n",
        "\n",
        "# 3) show python interpreter đang dùng\n",
        "import sys\n",
        "print(\"Python executable:\", sys.executable)\n",
        "\n",
        "# 4) dùng chính interpreter đó để cài (an toàn hơn dùng !pip trực tiếp)\n",
        "!{sys.executable} -m pip install -r vlsp-mt/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eleAhgayLH6A"
      },
      "source": [
        "# Run script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2MnSwH7YU0O"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDIBELveLUZ5",
        "outputId": "ce4e72d2-c02f-421b-a277-ee1a58b59407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BTL-NLP-2526I_INT3406_3/vlsp-mt\n"
          ]
        }
      ],
      "source": [
        "%cd vlsp-mt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu97wU9yYhNd"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F8kX8jqLTIc",
        "outputId": "1d9de5fa-d3ac-4ee4-951e-f1e3ed4620ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zFkkMbgJMSlh",
        "outputId": "d67ddd5a-8013-4674-baaa-c91a5a6ab79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 11:26:40.761667: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 11:26:40.779464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766402800.801291    6583 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766402800.807952    6583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766402800.824504    6583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766402800.824531    6583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766402800.824534    6583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766402800.824536    6583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 11:26:40.829430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading tokenizer: Qwen/Qwen2.5-3B-Instruct\n",
            "tokenizer_config.json: 7.30kB [00:00, 29.2MB/s]\n",
            "vocab.json: 2.78MB [00:00, 131MB/s]\n",
            "merges.txt: 1.67MB [00:00, 136MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 185MB/s]\n",
            "Loading medical vocabulary from: data/medical_vocab.txt\n",
            "[INFO] Added 96 medical tokens to vocabulary.\n",
            "[INFO] New vocab size: 151761\n",
            "Loading model: Qwen/Qwen2.5-3B-Instruct\n",
            "Using dtype: torch.bfloat16\n",
            "Flash Attention not installed, using SDPA\n",
            "config.json: 100% 661/661 [00:00<00:00, 6.81MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 35.6kB [00:00, 59.5MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/3.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 866k/2.20G [00:02<1:26:24, 425kB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 17.4M/2.20G [00:02<03:19, 11.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 51.6M/2.20G [00:02<00:57, 37.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 20.3k/3.97G [00:02<130:04:38, 8.47kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 85.3M/2.20G [00:02<00:32, 65.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 92.9M/3.97G [00:02<01:17, 49.9MB/s]    \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 125M/3.97G [00:02<00:58, 65.6MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 155M/3.97G [00:02<00:44, 85.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 191M/3.97G [00:02<00:33, 113MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 114M/2.20G [00:03<00:35, 58.5MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 355M/2.20G [00:03<00:06, 287MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 262M/3.97G [00:03<00:23, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 389M/3.97G [00:03<00:12, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 532M/3.97G [00:03<00:07, 465MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 718M/3.97G [00:03<00:04, 713MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 841M/3.97G [00:03<00:04, 736MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 467M/2.20G [00:03<00:08, 216MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.02G/3.97G [00:03<00:03, 867MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.16G/3.97G [00:04<00:02, 946MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 672M/2.20G [00:04<00:04, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 801M/2.20G [00:04<00:03, 449MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 900M/2.20G [00:04<00:02, 510MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 988M/2.20G [00:04<00:02, 537MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.12G/2.20G [00:04<00:01, 676MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.22G/2.20G [00:04<00:01, 507MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.29G/3.97G [00:04<00:07, 376MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.36G/2.20G [00:04<00:01, 627MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 1.65G/3.97G [00:05<00:03, 658MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 1.77G/3.97G [00:05<00:03, 649MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 1.92G/3.97G [00:05<00:02, 747MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.05G/3.97G [00:05<00:02, 733MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.51G/2.20G [00:05<00:01, 406MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.16G/3.97G [00:05<00:02, 757MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.26G/3.97G [00:05<00:02, 649MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.60G/2.20G [00:06<00:01, 332MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.37G/3.97G [00:06<00:03, 432MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 1.66G/2.20G [00:06<00:01, 273MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 2.44G/3.97G [00:06<00:05, 302MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 1.82G/2.20G [00:07<00:01, 278MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 2.49G/3.97G [00:07<00:04, 308MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 2.53G/3.97G [00:07<00:04, 317MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 2.58G/3.97G [00:07<00:04, 330MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 1.89G/2.20G [00:07<00:01, 254MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 2.66G/3.97G [00:09<00:14, 88.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.94G/2.20G [00:09<00:02, 95.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 2.81G/3.97G [00:09<00:07, 163MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.20G/2.20G [00:09<00:00, 228MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 2.99G/3.97G [00:09<00:03, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.12G/3.97G [00:09<00:02, 350MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 3.30G/3.97G [00:10<00:01, 439MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 3.40G/3.97G [00:10<00:01, 423MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 3.54G/3.97G [00:10<00:01, 421MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 3.64G/3.97G [00:10<00:00, 416MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 3.71G/3.97G [00:11<00:00, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 3.78G/3.97G [00:11<00:00, 405MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 3.83G/3.97G [00:11<00:00, 399MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 3.90G/3.97G [00:12<00:00, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 3.97G/3.97G [00:12<00:00, 308MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:13<00:00,  6.70s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.07it/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 488kB/s]\n",
            "[INFO] Resizing model embeddings from 151936 to 151761\n",
            "[INFO] Initializing new embeddings with average of existing embeddings...\n",
            "Gradient checkpointing DISABLED (faster but more VRAM)\n",
            "Trainable params: 59,867,136 (1.90%)\n",
            "Loading training data...\n",
            "Training samples: 334812\n",
            "Tokenizing train: 100% 334812/334812 [06:33<00:00, 850.31 examples/s]\n",
            "Loading validation data...\n",
            "Validation samples: 1000\n",
            "Tokenizing val: 100% 1000/1000 [00:01<00:00, 694.29 examples/s]\n",
            "Effective batch size: 48\n",
            "Steps per epoch: 6975\n",
            "Total steps: 13950\n",
            "Eval/Save every: 3000 steps\n",
            "Early stopping enabled with patience=3\n",
            "Loading BLEU + chrF metrics for evaluation...\n",
            "Downloading builder script: 5.94kB [00:00, 17.8MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 14.6MB/s]       \n",
            "Downloading extra modules: 3.34kB [00:00, 15.8MB/s]\n",
            "Downloading builder script: 9.01kB [00:00, 22.0MB/s]\n",
            "NEFTune enabled with alpha=3.0\n",
            "BLEU + chrF evaluation enabled (sample_size=100)\n",
            "/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/train_qwen_lora.py:325: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NEFTuneBLEUTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(**kwargs)\n",
            "\n",
            "============================================================\n",
            "Starting training...\n",
            "============================================================\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "{'loss': 2.4705, 'grad_norm': 21.056041717529297, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 1.1277, 'grad_norm': 2.359269142150879, 'learning_rate': 0.0001498449924923562, 'epoch': 0.1}\n",
            "{'loss': 0.9739, 'grad_norm': 2.1955833435058594, 'learning_rate': 0.0001480909833546549, 'epoch': 0.2}\n",
            "{'loss': 0.9378, 'grad_norm': 2.19927716255188, 'learning_rate': 0.0001444275904154545, 'epoch': 0.3}\n",
            "{'loss': 0.9203, 'grad_norm': 2.2738654613494873, 'learning_rate': 0.00013895051389406507, 'epoch': 0.4}\n",
            " 22% 3000/13952 [1:20:22<4:42:53,  1.55s/it]\n",
            "  0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 2/21 [00:00<00:05,  3.21it/s]\u001b[A\n",
            " 14% 3/21 [00:01<00:08,  2.22it/s]\u001b[A\n",
            " 19% 4/21 [00:02<00:09,  1.78it/s]\u001b[A\n",
            " 24% 5/21 [00:02<00:10,  1.58it/s]\u001b[A\n",
            " 29% 6/21 [00:03<00:10,  1.48it/s]\u001b[A\n",
            " 33% 7/21 [00:04<00:09,  1.42it/s]\u001b[A\n",
            " 38% 8/21 [00:05<00:09,  1.42it/s]\u001b[A\n",
            " 43% 9/21 [00:05<00:08,  1.41it/s]\u001b[A\n",
            " 48% 10/21 [00:06<00:07,  1.38it/s]\u001b[A\n",
            " 52% 11/21 [00:07<00:07,  1.36it/s]\u001b[A\n",
            " 57% 12/21 [00:08<00:06,  1.34it/s]\u001b[A\n",
            " 62% 13/21 [00:08<00:05,  1.34it/s]\u001b[A\n",
            " 67% 14/21 [00:09<00:05,  1.34it/s]\u001b[A\n",
            " 71% 15/21 [00:10<00:04,  1.33it/s]\u001b[A\n",
            " 76% 16/21 [00:11<00:03,  1.34it/s]\u001b[A\n",
            " 81% 17/21 [00:11<00:02,  1.34it/s]\u001b[A\n",
            " 86% 18/21 [00:12<00:02,  1.33it/s]\u001b[A\n",
            " 90% 19/21 [00:13<00:01,  1.37it/s]\u001b[A\n",
            " 95% 20/21 [00:13<00:00,  1.39it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.904703676700592, 'eval_runtime': 15.4089, 'eval_samples_per_second': 64.898, 'eval_steps_per_second': 1.363, 'epoch': 0.43}\n",
            " 22% 3000/13952 [1:20:37<4:42:53,  1.55s/it]\n",
            "100% 21/21 [00:14<00:00,  1.45it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'loss': 0.8953, 'grad_norm': 2.0490903854370117, 'learning_rate': 0.000131802833561883, 'epoch': 0.5}\n",
            "{'loss': 0.8829, 'grad_norm': 2.0314037799835205, 'learning_rate': 0.000123171271014626, 'epoch': 0.6}\n",
            "{'loss': 0.8678, 'grad_norm': 1.990687370300293, 'learning_rate': 0.00011328131187244763, 'epoch': 0.7}\n",
            "{'loss': 0.8521, 'grad_norm': 2.0173001289367676, 'learning_rate': 0.0001023913153325684, 'epoch': 0.8}\n",
            " 43% 6000/13952 [2:41:18<3:35:56,  1.63s/it]\n",
            "  0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 2/21 [00:00<00:05,  3.21it/s]\u001b[A\n",
            " 14% 3/21 [00:01<00:08,  2.22it/s]\u001b[A\n",
            " 19% 4/21 [00:02<00:09,  1.78it/s]\u001b[A\n",
            " 24% 5/21 [00:02<00:10,  1.58it/s]\u001b[A\n",
            " 29% 6/21 [00:03<00:10,  1.48it/s]\u001b[A\n",
            " 33% 7/21 [00:04<00:09,  1.42it/s]\u001b[A\n",
            " 38% 8/21 [00:05<00:09,  1.42it/s]\u001b[A\n",
            " 43% 9/21 [00:05<00:08,  1.41it/s]\u001b[A\n",
            " 48% 10/21 [00:06<00:07,  1.38it/s]\u001b[A\n",
            " 52% 11/21 [00:07<00:07,  1.36it/s]\u001b[A\n",
            " 57% 12/21 [00:08<00:06,  1.34it/s]\u001b[A\n",
            " 62% 13/21 [00:08<00:05,  1.34it/s]\u001b[A\n",
            " 67% 14/21 [00:09<00:05,  1.34it/s]\u001b[A\n",
            " 71% 15/21 [00:10<00:04,  1.33it/s]\u001b[A\n",
            " 76% 16/21 [00:11<00:03,  1.34it/s]\u001b[A\n",
            " 81% 17/21 [00:11<00:02,  1.34it/s]\u001b[A\n",
            " 86% 18/21 [00:12<00:02,  1.33it/s]\u001b[A\n",
            " 90% 19/21 [00:13<00:01,  1.37it/s]\u001b[A\n",
            " 95% 20/21 [00:13<00:00,  1.39it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.8384510278701782, 'eval_runtime': 15.3951, 'eval_samples_per_second': 64.956, 'eval_steps_per_second': 1.364, 'epoch': 0.86}\n",
            " 43% 6000/13952 [2:41:33<3:35:56,  1.63s/it]\n",
            "100% 21/21 [00:14<00:00,  1.45it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'loss': 0.8331, 'grad_norm': 1.8295801877975464, 'learning_rate': 9.078576495284019e-05, 'epoch': 0.9}\n",
            "{'loss': 0.8204, 'grad_norm': 2.0260674953460693, 'learning_rate': 7.87678369786148e-05, 'epoch': 1.0}\n",
            "{'loss': 0.6874, 'grad_norm': 2.0220882892608643, 'learning_rate': 6.66514803533649e-05, 'epoch': 1.1}\n",
            "{'loss': 0.6783, 'grad_norm': 1.9119216203689575, 'learning_rate': 5.475321530997855e-05, 'epoch': 1.2}\n",
            " 65% 9000/13952 [4:02:21<2:08:53,  1.56s/it]\n",
            "  0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 2/21 [00:00<00:03,  5.97it/s]\u001b[A\n",
            " 14% 3/21 [00:00<00:06,  2.71it/s]\u001b[A\n",
            " 19% 4/21 [00:01<00:08,  1.95it/s]\u001b[A\n",
            " 24% 5/21 [00:02<00:09,  1.67it/s]\u001b[A\n",
            " 29% 6/21 [00:03<00:09,  1.53it/s]\u001b[A\n",
            " 33% 7/21 [00:04<00:09,  1.45it/s]\u001b[A\n",
            " 38% 8/21 [00:04<00:09,  1.44it/s]\u001b[A\n",
            " 43% 9/21 [00:05<00:08,  1.42it/s]\u001b[A\n",
            " 48% 10/21 [00:06<00:07,  1.38it/s]\u001b[A\n",
            " 52% 11/21 [00:06<00:07,  1.36it/s]\u001b[A\n",
            " 57% 12/21 [00:07<00:06,  1.35it/s]\u001b[A\n",
            " 62% 13/21 [00:08<00:05,  1.35it/s]\u001b[A\n",
            " 67% 14/21 [00:09<00:05,  1.34it/s]\u001b[A\n",
            " 71% 15/21 [00:10<00:04,  1.33it/s]\u001b[A\n",
            " 76% 16/21 [00:10<00:03,  1.34it/s]\u001b[A\n",
            " 81% 17/21 [00:11<00:02,  1.34it/s]\u001b[A\n",
            " 86% 18/21 [00:12<00:02,  1.33it/s]\u001b[A\n",
            " 90% 19/21 [00:12<00:01,  1.37it/s]\u001b[A\n",
            " 95% 20/21 [00:13<00:00,  1.39it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.8001223206520081, 'eval_runtime': 15.8816, 'eval_samples_per_second': 62.966, 'eval_steps_per_second': 1.322, 'epoch': 1.29}\n",
            " 65% 9000/13952 [4:02:37<2:08:53,  1.56s/it]\n",
            "100% 21/21 [00:14<00:00,  1.45it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'loss': 0.6688, 'grad_norm': 1.914908766746521, 'learning_rate': 4.338386479127422e-05, 'epoch': 1.3}\n",
            "{'loss': 0.6671, 'grad_norm': 1.7317413091659546, 'learning_rate': 3.284043470300971e-05, 'epoch': 1.4}\n",
            "{'loss': 0.6583, 'grad_norm': 1.7132515907287598, 'learning_rate': 2.339835511465184e-05, 'epoch': 1.5}\n",
            "{'loss': 0.656, 'grad_norm': 1.7511179447174072, 'learning_rate': 1.5304285093995582e-05, 'epoch': 1.6}\n",
            "{'loss': 0.6429, 'grad_norm': 1.9363008737564087, 'learning_rate': 8.769669137698626e-06, 'epoch': 1.7}\n",
            " 86% 12000/13952 [5:23:17<49:04,  1.51s/it]\n",
            "  0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 2/21 [00:00<00:03,  5.97it/s]\u001b[A\n",
            " 14% 3/21 [00:00<00:06,  2.70it/s]\u001b[A\n",
            " 19% 4/21 [00:01<00:08,  1.95it/s]\u001b[A\n",
            " 24% 5/21 [00:02<00:09,  1.67it/s]\u001b[A\n",
            " 29% 6/21 [00:03<00:09,  1.53it/s]\u001b[A\n",
            " 33% 7/21 [00:04<00:09,  1.45it/s]\u001b[A\n",
            " 38% 8/21 [00:04<00:09,  1.44it/s]\u001b[A\n",
            " 43% 9/21 [00:05<00:08,  1.42it/s]\u001b[A\n",
            " 48% 10/21 [00:06<00:07,  1.38it/s]\u001b[A\n",
            " 52% 11/21 [00:06<00:07,  1.36it/s]\u001b[A\n",
            " 57% 12/21 [00:07<00:06,  1.34it/s]\u001b[A\n",
            " 62% 13/21 [00:08<00:05,  1.34it/s]\u001b[A\n",
            " 67% 14/21 [00:09<00:05,  1.34it/s]\u001b[A\n",
            " 71% 15/21 [00:10<00:04,  1.33it/s]\u001b[A\n",
            " 76% 16/21 [00:10<00:03,  1.34it/s]\u001b[A\n",
            " 81% 17/21 [00:11<00:02,  1.34it/s]\u001b[A\n",
            " 86% 18/21 [00:12<00:02,  1.33it/s]\u001b[A\n",
            " 90% 19/21 [00:12<00:01,  1.36it/s]\u001b[A\n",
            " 95% 20/21 [00:13<00:00,  1.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.7681640982627869, 'eval_runtime': 15.8991, 'eval_samples_per_second': 62.897, 'eval_steps_per_second': 1.321, 'epoch': 1.72}\n",
            " 86% 12000/13952 [5:23:33<49:04,  1.51s/it]\n",
            "100% 21/21 [00:14<00:00,  1.45it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'loss': 0.643, 'grad_norm': 2.1607487201690674, 'learning_rate': 3.9652135255687974e-06, 'epoch': 1.8}\n",
            "{'loss': 0.6431, 'grad_norm': 1.801727056503296, 'learning_rate': 1.0164268949206745e-06, 'epoch': 1.9}\n",
            "{'loss': 0.6409, 'grad_norm': 1.7685045003890991, 'learning_rate': 3.415302881004667e-10, 'epoch': 2.0}\n",
            "100% 13952/13952 [6:16:24<00:00,  1.31s/it]/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'train_runtime': 22589.8108, 'train_samples_per_second': 29.643, 'train_steps_per_second': 0.618, 'train_loss': 0.7848372149768226, 'epoch': 2.0}\n",
            "100% 13952/13952 [6:16:29<00:00,  1.62s/it]\n",
            "\n",
            "Saving adapter to runs/en2vi_v1/lora_en2vi_sft\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "\n",
            "============================================================\n",
            "Training complete!\n",
            "Adapter saved to: runs/en2vi_v1/lora_en2vi_sft\n",
            "Metadata saved to: runs/en2vi_v1/meta.json\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!python scripts/train_qwen_lora.py \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --direction en2vi \\\n",
        "  --src data/dedup/train.en \\\n",
        "  --tgt data/dedup/train.vi \\\n",
        "  --val_src data/clean/dev.en \\\n",
        "  --val_tgt data/clean/dev.vi \\\n",
        "  --run_id en2vi_v1 \\\n",
        "  --epochs 2 \\\n",
        "  --batch_size 24 \\\n",
        "  --grad_accum 2 \\\n",
        "  --lr 1.5e-4 \\\n",
        "  --lora_r 32 \\\n",
        "  --lora_alpha 64 \\\n",
        "  --neftune_alpha 3.0 \\\n",
        "  --eval_bleu \\\n",
        "  --bleu_sample_size 100 \\\n",
        "  --eval_steps 3000 \\\n",
        "  --early_stopping_patience 3 \\\n",
        "  --no_grad_checkpoint \\\n",
        "  --max_len 256 \\\n",
        "  --medical_vocab data/medical_vocab.txt \\\n",
        "  --init_new_embeddings_avg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tạo thư mục đích trước\n",
        "!mkdir -p data/rl_subset\n",
        "\n",
        "# 2. Chạy lệnh cắt file\n",
        "!head -n 20000 data/dedup/train.en > data/rl_subset/en.txt\n",
        "!head -n 20000 data/dedup/train.vi > data/rl_subset/vi.txt\n",
        "\n",
        "!ls -lh data/rl_subset/"
      ],
      "metadata": {
        "id": "o98ny3rl-uKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae2b3b9-412c-41e6-b900-7c745097c6bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 6.3M\n",
            "-rw-r--r-- 1 root root 2.8M Dec 22 17:50 en.txt\n",
            "-rw-r--r-- 1 root root 3.6M Dec 22 17:50 vi.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "uKL8a2LJMeGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d88ebf-f725-4428-8cf1-491c363930b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 17:50:47.449830: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 17:50:47.468074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766425847.490524  100330 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766425847.497189  100330 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766425847.514128  100330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425847.514173  100330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425847.514176  100330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425847.514179  100330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 17:50:47.519211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using device: cuda\n",
            "Loaded tokenizer from adapter: runs/en2vi_v1/lora_en2vi_sft\n",
            "Tokenizer vocab size: 151761\n",
            "Using SDPA\n",
            "Loading base model (single instance)...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.06s/it]\n",
            "Resizing embeddings: 151936 -> 151761\n",
            "Loading policy adapter from: runs/en2vi_v1/lora_en2vi_sft\n",
            "Loading reference adapter from: runs/en2vi_v1/lora_en2vi_sft\n",
            "Trainable params: 59,867,136\n",
            "Loaded RL data: 20000 pairs\n",
            "Total training steps: 625\n",
            "Epoch 1/1:   0% 0/5000 [00:00<?, ?it/s]/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/rl_train_grpo.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/rl_train_grpo.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "Epoch 1/1:   0% 10/5000 [01:21<11:17:37,  8.15s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2564, in generate\n",
            "    result = decoding_method(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2787, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python scripts/rl_train_grpo.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --sft_adapter runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --init_adapter runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --rl_src data/rl_subset/en.txt \\\n",
        "    --rl_tgt data/rl_subset/vi.txt \\\n",
        "    --run_id en2vi_v1_rl \\\n",
        "    --direction en2vi \\\n",
        "    --epochs 1 \\\n",
        "    --batch_size 8 \\\n",
        "    --grad_accum_steps 8 \\\n",
        "    --lr 3e-6 \\\n",
        "    --kl_coef 0.02 \\\n",
        "    --temperature 0.8 \\\n",
        "    --max_new_tokens 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# augemnet"
      ],
      "metadata": {
        "id": "yijECkXn-6vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/back_translate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/vi2en_v1/lora_vi2en_sft \\\n",
        "    --input data/rl_subset/vi.txt \\\n",
        "    --output data/augment/bt.en \\\n",
        "    --direction vi2en \\\n",
        "    --batch_size 16 \\\n",
        "    --temperature 0.7"
      ],
      "metadata": {
        "id": "nJXm6w5Z-8-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUWLD0JqyK8",
        "outputId": "cadc7953-ffbc-4245-bbfe-2288f33ff364"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".   data     REPORT.md\t       RUN_GUIDE.md  runs\n",
            "..  outputs  requirements.txt  run.md\t     scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls runs/vi2en_v1/lora_vi2en_sft/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s27Lpq7eqscS",
        "outputId": "aafe12db-16dd-4b9d-9bde-e9905d288b71"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'runs/vi2en_v1/lora_vi2en_sft/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/back_translate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft/ \\\n",
        "    --input data/rl_subset/vi.txt \\\n",
        "    --output data/augment/bt.en \\\n",
        "    --direction vi2en \\\n",
        "    --batch_size 16 \\\n",
        "    --temperature 0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYsrTZf0p79j",
        "outputId": "47d0d0da-4c5a-483c-a862-1f0a3977219e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 20:02:25.741340: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 20:02:25.759918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766433745.782401  134222 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766433745.789088  134222 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766433745.806149  134222 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766433745.806189  134222 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766433745.806192  134222 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766433745.806195  134222 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 20:02:25.811330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading tokenizer from adapter: runs/vi2en_v1/lora_vi2en_sft\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'runs/vi2en_v1/lora_vi2en_sft'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/back_translate.py\", line 209, in <module>\n",
            "    main()\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/back_translate.py\", line 101, in main\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.adapter_path, use_fast=False)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1089, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 921, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 532, in cached_files\n",
            "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 143, in _get_cache_file_to_return\n",
            "    resolved_file = try_to_load_from_cache(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'runs/vi2en_v1/lora_vi2en_sft'. Use `repo_type` argument if needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/clean/test.en data/augment/bt.en > data/augment/train_aug_en2vi.en\n",
        "!cat data/clean/test.vi data/rl_subset/vi.txt > data/augment/train_aug_en2vi.vi"
      ],
      "metadata": {
        "id": "5tOqU5KA_Y7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/clean/test.en data/augment/bt.en > data/augment/train_aug_en2vi.en\n",
        "!cat data/clean/test.vi data/rl_subset/vi.txt > data/augment/train_aug_en2vi.vi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9mXGxmZp-jr",
        "outputId": "831cfdbf-3e43-4598-d913-8e818b4cd930"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: data/augment/train_aug_en2vi.en: No such file or directory\n",
            "/bin/bash: line 1: data/augment/train_aug_en2vi.vi: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/train_qwen_lora.py \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --direction en2vi \\\n",
        "  --src data/augment/train_aug.en \\\n",
        "  --tgt data/augment/train_aug.vi \\\n",
        "  --val_src data/clean/dev.en \\\n",
        "  --val_tgt data/clean/dev.vi \\\n",
        "  --run_id en2vi_v2_aug \\\n",
        "  --epochs 2 \\\n",
        "  --batch_size 24 \\\n",
        "  --grad_accum 2 \\\n",
        "  --lr 1.5e-4 \\\n",
        "  --lora_r 32 \\\n",
        "  --lora_alpha 64 \\\n",
        "  --neftune_alpha 3.0 \\\n",
        "  --eval_bleu \\\n",
        "  --bleu_sample_size 100 \\\n",
        "  --eval_steps 1000 \\\n",
        "  --early_stopping_patience 3 \\\n",
        "  --max_len 256 \\\n",
        "  --medical_vocab data/medical_vocab.txt \\\n",
        "  --init_new_embeddings_avg"
      ],
      "metadata": {
        "id": "Vm_hQ8PF_igj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af32185-bcdb-4147-b6d9-1472b307e1db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 17:52:45.553470: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 17:52:45.570978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766425965.593288  100934 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766425965.599952  100934 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766425965.616450  100934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425965.616499  100934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425965.616502  100934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766425965.616505  100934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 17:52:45.621319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/oauth2client/client.py\", line 45, in <module>\n",
            "    from oauth2client import crypt\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/oauth2client/crypt.py\", line 23, in <module>\n",
            "    from oauth2client import _pure_python_crypt\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/oauth2client/_pure_python_crypt.py\", line 25, in <module>\n",
            "    from pyasn1_modules.rfc5208 import PrivateKeyInfo\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1_modules/rfc5208.py\", line 14, in <module>\n",
            "    from pyasn1_modules import rfc2251\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1_modules/rfc2251.py\", line 257, in <module>\n",
            "    class SearchRequest(univ.Sequence):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1_modules/rfc2251.py\", line 261, in SearchRequest\n",
            "    componentType = namedtype.NamedTypes(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 154, in __init__\n",
            "    self.__uniqueTagMap = self.__computeTagMaps(unique=True)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 471, in __computeTagMaps\n",
            "    return NamedTypes.PostponedError('Non-unique tagSet %s of %s at %s' % (tagSet, namedType, self))\n",
            "                                                                                              ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 169, in __repr__\n",
            "    representation = ', '.join(['%r' % x for x in self.__namedTypes])\n",
            "                                ~~~~~^~~\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 42, in __repr__\n",
            "    representation = '%s=%r' % (self.name, self.asn1Object)\n",
            "                                           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "                                         ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 169, in __repr__\n",
            "    representation = ', '.join(['%r' % x for x in self.__namedTypes])\n",
            "                                ~~~~~^~~\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 42, in __repr__\n",
            "    representation = '%s=%r' % (self.name, self.asn1Object)\n",
            "                                           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "                                         ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "                                         ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 169, in __repr__\n",
            "    representation = ', '.join(['%r' % x for x in self.__namedTypes])\n",
            "                                ~~~~~^~~\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 42, in __repr__\n",
            "    representation = '%s=%r' % (self.name, self.asn1Object)\n",
            "                                           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "                                         ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "                                         ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 169, in __repr__\n",
            "    representation = ', '.join(['%r' % x for x in self.__namedTypes])\n",
            "                                ~~~~~^~~\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/namedtype.py\", line 42, in __repr__\n",
            "    representation = '%s=%r' % (self.name, self.asn1Object)\n",
            "                                           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyasn1/type/base.py\", line 537, in __repr__\n",
            "    representation += ', %s=%r' % (attr, value)\n",
            "    ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/train_qwen_lora.py\", line 10, in <module>\n",
            "    from transformers import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 42, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\", line 44, in <module>\n",
            "    from .. import PreTrainedModel, TrainingArguments\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 70, in <module>\n",
            "    from .loss.loss_utils import LOSS_MAPPING\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\", line 21, in <module>\n",
            "    from .loss_d_fine import DFineForObjectDetectionLoss\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_d_fine.py\", line 21, in <module>\n",
            "    from .loss_for_object_detection import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_for_object_detection.py\", line 32, in <module>\n",
            "    from transformers.image_transforms import center_to_corners_format\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\", line 49, in <module>\n",
            "    from tensorflow._api.v2 import __internal__\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
            "    from tensorflow._api.v2.__internal__ import distribute\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.python.distribute.combinations import env # line: 456\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
            "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 33, in <module>\n",
            "    from tensorflow.python.distribute import mirrored_strategy\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 34, in <module>\n",
            "    from tensorflow.python.distribute.cluster_resolver import tfconfig_cluster_resolver\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/cluster_resolver/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.distribute.cluster_resolver.gce_cluster_resolver import GCEClusterResolver\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py\", line 24, in <module>\n",
            "    from googleapiclient import discovery  # pylint: disable=g-import-not-at-top\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/googleapiclient/discovery.py\", line 64, in <module>\n",
            "    from googleapiclient import _auth, mimeparse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/googleapiclient/_auth.py\", line 34, in <module>\n",
            "    import oauth2client.client\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1334, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVEdWvfNa5rg"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReGWjQNvMkeV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/clean/test.en \\\n",
        "    --output outputs/test_en2vi.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcesgjccMs5g",
        "outputId": "b691c304-3d8c-4896-abe7-71e9cc54d78e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hypothesis: outputs/test_en2vi.hyp.vi\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 477, in <module>\n",
            "    main()\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 374, in main\n",
            "    hyp = load_file(args.hyp)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 47, in load_file\n",
            "    with open(path, encoding=\"utf8\") as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'outputs/test_en2vi.hyp.vi'\n"
          ]
        }
      ],
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/test_en2vi.hyp.vi \\\n",
        "    --ref data/clean/test.vi \\\n",
        "    --src data/clean/test.en \\\n",
        "    --show_worst 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RAZ1n2enHAtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CUPimiQmMrcl"
      },
      "outputs": [],
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1_rl/final_model \\\n",
        "    --direction en2vi \\\n",
        "    --input data/clean/test.en \\\n",
        "    --output outputs/test_en2vi_rl.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI78HtAcMv73",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/test_en2vi_rl.hyp.vi \\\n",
        "    --ref data/clean/test.vi \\\n",
        "    --src data/clean/test.en \\\n",
        "    --show_worst 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvR5GegmHz_v"
      },
      "source": [
        "# Public test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZetu5FxH1-C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/raw/public_test.en.txt \\\n",
        "    --output outputs/public_test_en2vi_sft.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/raw/public_test.en.txt \\\n",
        "    --output outputs/public_test_en2vi_sft.hyp.vi \\\n",
        "    --batch_size 24 \\\n",
        "    --num_beams 5 \\\n",
        "    --repetition_penalty 1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34L9NHXHZL0i",
        "outputId": "cfe1e86f-b123-4a72-b18f-7029d4dc6604"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 19:32:09.960918: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 19:32:09.979297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766431930.001610  126631 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766431930.008239  126631 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766431930.025273  126631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766431930.025312  126631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766431930.025315  126631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766431930.025319  126631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 19:32:10.030604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model: Qwen/Qwen2.5-3B-Instruct\n",
            "Loading adapter: runs/en2vi_v1/lora_en2vi_sft\n",
            "Loaded tokenizer from adapter (may include medical vocab)\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.06it/s]\n",
            "Resizing embeddings: 151936 -> 151761\n",
            "Model loaded on cuda:0, dtype=torch.bfloat16\n",
            "Loaded 3000 sentences from data/raw/public_test.en.txt\n",
            "Generating:   0% 0/125 [00:00<?, ?it/s]/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/generate.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating: 100% 125/125 [27:47<00:00, 13.34s/it]\n",
            "\n",
            "Saved 3000 translations to outputs/public_test_en2vi_sft.hyp.vi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/public_test_en2vi_sft.hyp.vi \\\n",
        "    --ref data/raw/public_test.vi.txt \\\n",
        "    --src data/raw/public_test.en.txt \\\n",
        "    --show_worst 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI8xQLTWZM9L",
        "outputId": "3cc62a73-80b6-4eee-a07f-04bfbaa42571"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hypothesis: outputs/public_test_en2vi_sft.hyp.vi\n",
            "Loading reference: data/raw/public_test.vi.txt\n",
            "Loading source: data/raw/public_test.en.txt\n",
            "\n",
            "Evaluating 3000 sentence pairs...\n",
            "Downloading NLTK resource: wordnet\n",
            "Downloading NLTK resource: omw-1.4\n",
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS\n",
            "==================================================\n",
            "  BLEU:    52.78\n",
            "  chrF++:  66.13\n",
            "  chrF:    66.26\n",
            "  TER:     40.87 (lower is better)\n",
            "  METEOR:  74.84\n",
            "==================================================\n",
            "\n",
            "10 WORST TRANSLATIONS (by BLEU):\n",
            "--------------------------------------------------\n",
            "\n",
            "[1] Line 2879 | BLEU: 3.8 | chrF: 16.1\n",
            "  SRC: Severe respiratory failure was observed on 56% of PHOCD and on 27% of PHCD.\n",
            "  HYP: 56% trẻ TAP nặng và 27% trẻ TAP không nặng có suy thông khí mức độ nặng.\n",
            "  REF: Khởi phát bệnh sớm và biểu hiện suy hô hấp nặng ở nhóm TAP không liên quan tim cao hơn nhóm TAP liên quan tim (56% so với 27%).\n",
            "\n",
            "[2] Line 1858 | BLEU: 4.9 | chrF: 30.0\n",
            "  SRC: 4 / No side effects are found in patients using Hairy antler.\n",
            "  HYP: 4 / Không có tác dụng không mong muốn nào được ghi nhận ở các bệnh nhân sử dụng Nhung nai.\n",
            "  REF: Chưa phát hiện tác dụng phụ khi dùng viên nhung nai.\n",
            "\n",
            "[3] Line 2508 | BLEU: 4.9 | chrF: 18.2\n",
            "  SRC: Hypothyroidism is severe and is seen in almost all the patients and hypocortisol state may be variable (Table-1).\n",
            "  HYP: Suy giáp nặng và được thấy ở hầu hết các bệnh nhân và trạng thái hypocortisol có thể thay đổi (Bảng 1).\n",
            "  REF: Suy tuyến yên và suy tế bào tiết hormone tuyến giáp được tìm thấy ở hầu hết bệnh nhân, còn các hormone còn lại của thuỳ trước tuyến yên trong một số trường hợp vẫn trong giới hạn bình thường được gọi là Hội chứng Sheehan cục bộ.\n",
            "\n",
            "[4] Line 1277 | BLEU: 5.2 | chrF: 41.5\n",
            "  SRC: The genotypes observed were in agreement with those expected under Hardy-Weinberg equilibrium.\n",
            "  HYP: Các kiểu gen thu được phù hợp với cân bằng Hardy-Weinberg.\n",
            "  REF: Sự phân bố kiểu gen ở quần thể nghiên cứu tuân theo quy luật cân bằng Hardy - Weinberg.\n",
            "\n",
            "[5] Line 2039 | BLEU: 5.2 | chrF: 31.6\n",
            "  SRC: In vascular imaging, this technique provides image quality that equals or surpasses that of conventional angiography.\n",
            "  HYP: Trong chẩn đoán hình ảnh mạch máu, kỹ thuật này cung cấp chất lượng hình ảnh tương đương hoặc vượt trội so với chụp động mạch quy ước.\n",
            "  REF: Trong kỹ thuật chụp mạch máu lớn của lồng ngực (động mạch chủ, động mạch phổi...), chụp cắt lớp điện toán đa lát cắt này cho được các hình ảnh bằng hoặc rõ hơn chụp mạch máu quy ước.\n",
            "\n",
            "[6] Line 2553 | BLEU: 5.2 | chrF: 15.6\n",
            "  SRC: Draining cutaneous fistulas may occur.\n",
            "  HYP: Rò da có thể xảy ra.\n",
            "  REF: Có thể xuất hiện đường rò dẫn lưu ra da.\n",
            "\n",
            "[7] Line 1588 | BLEU: 5.4 | chrF: 23.5\n",
            "  SRC: There are not any behaviors or suggestions related to bribery.\n",
            "  HYP: Không có bất kỳ hành vi hoặc đề xuất nào liên quan đến tham nhũng.\n",
            "  REF: Không có lời nói, cử chỉ gợi ý tiên quà người bệnh.\n",
            "\n",
            "[8] Line 2003 | BLEU: 5.4 | chrF: 22.0\n",
            "  SRC: Isolation of chitin from field crap shells and production of chitosan using decrystallization method\n",
            "  HYP: Phân lập và sản xuất chitosan từ vỏ ngao dầu bằng phương pháp khử kết tinh\n",
            "  REF: Chế tạo glucosamine hydrochloride từ chitosan tách từ vỏ cua đồng\n",
            "\n",
            "[9] Line 1927 | BLEU: 5.5 | chrF: 26.3\n",
            "  SRC: Rate of head injury in time - when the study has done decreased.\n",
            "  HYP: Tỷ lệ chấn thương đầu trong thời gian nghiên cứu giảm dần.\n",
            "  REF: Tỷ lệ chấn thương sọ não trong thời gian khảo sát từ sau khi Nghị quyết 32 chính thức có hiệu lực và đi vào cuộc sống giảm so với cùng kỳ năm trước.\n",
            "\n",
            "[10] Line 1365 | BLEU: 5.7 | chrF: 36.6\n",
            "  SRC: Copper to ceruloplasmin is found to be relative to alcohol consumption.\n",
            "  HYP: Có mối liên quan giữa nồng độ đồng và ceruloplasmin máu với lượng rượu tiêu thụ.\n",
            "  REF: Tỉ số đồng / ceruloplasmin huyết thanh có liên quan đến tình trạng sử dụng rượu bia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FOaOc-pH15i",
        "outputId": "d77288fa-f9c3-412d-d323-8f05f0f0896e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hypothesis: outputs/public_test_en2vi_sft.hyp.vi\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 477, in <module>\n",
            "    main()\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 374, in main\n",
            "    hyp = load_file(args.hyp)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/eval_bleu.py\", line 47, in load_file\n",
            "    with open(path, encoding=\"utf8\") as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'outputs/public_test_en2vi_sft.hyp.vi'\n"
          ]
        }
      ],
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/public_test_en2vi_sft.hyp.vi \\\n",
        "    --ref data/raw/public_test.vi.txt \\\n",
        "    --src data/raw/public_test.en.txt \\\n",
        "    --show_worst 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cb0wSxbaHv3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1_rl/final_model \\\n",
        "    --direction en2vi \\\n",
        "    --input data/raw/public_test.en.txt \\\n",
        "    --output outputs/public_test_en2vi_rl.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ],
      "metadata": {
        "id": "VIIK5obVGLv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/public_test_en2vi_rl.hyp.vi \\\n",
        "    --ref data/raw/public_test.vi.txt \\\n",
        "    --src data/raw/public_test.en.txt \\\n",
        "    --show_worst 10"
      ],
      "metadata": {
        "id": "fdSTDz5YGLnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uRfZ5meaH9Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v2_aug/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/raw/public_test.en.txt \\\n",
        "    --output outputs/public_test_en2vi_aug.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ckgWXfPGM6p",
        "outputId": "5d21ddc0-359a-416b-d855-6858c19cd6d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object address  : 0x7d958b9041c0\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/public_test_en2vi_aug.hyp.vi \\\n",
        "    --ref data/raw/public_test.vi.txt \\\n",
        "    --src data/raw/public_test.en.txt \\\n",
        "    --show_worst 10"
      ],
      "metadata": {
        "id": "7yPvwXYtGM30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZmr7wuWHxxu"
      },
      "source": [
        "# Down"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "# Token HuggingFace\n",
        "hf_token = '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'\n",
        "login(hf_token)\n",
        "\n",
        "# Cấu hình\n",
        "local_folder = \"vlsp-mt/runs\"\n",
        "repo_name = \"tuan243/adapter-loRA-vlsp-mt\"\n",
        "\n",
        "# Upload\n",
        "api = HfApi()\n",
        "\n",
        "print(f\"Đang tạo repo '{repo_name}'...\")\n",
        "api.create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "print(f\"Đang đẩy model từ '{local_folder}' lên '{repo_name}'...\")\n",
        "api.upload_folder(\n",
        "    folder_path=local_folder,\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"✅ Xong! Link: https://huggingface.co/\" + repo_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "SUUnTa_WA1c2",
        "outputId": "a7fc24b3-9cd9-4791-f58c-c3a5cc4af4cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang tạo repo 'tuan243/adapter-loRA-vlsp-mt'...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-497978426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Đang tạo repo '{repo_name}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Đang đẩy model từ '{local_folder}' lên '{repo_name}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3743\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resourceGroupId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_group_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_build_hf_headers\u001b[0;34m(self, token, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   9516\u001b[0m             \u001b[0;31m# Cannot do `token = token or self.token` as token can be `False`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9517\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9518\u001b[0;31m         return build_hf_headers(\n\u001b[0m\u001b[1;32m   9519\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9520\u001b[0m             \u001b[0mlibrary_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlibrary_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Get auth token to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtoken_to_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_to_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Combine headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Token is not provided: we get it from local cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mcached_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Case token is explicitly required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_token_from_google_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_token_from_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_token_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py\u001b[0m in \u001b[0;36m_get_token_from_google_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0m_GOOGLE_COLAB_SECRET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m# thread-safe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0m_userdata_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     resp = _message.blocking_request(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;34m'GetSecret'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rMCoxNXGtT_Z"
      },
      "outputs": [],
      "source": [
        "!zip -r data12_backup.zip vlsp-mt/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gKijaNjz3s5"
      },
      "outputs": [],
      "source": [
        "# Copy file backup vào Google Drive\n",
        "!cp data12_backup.zip /content/drive/MyDrive/\n",
        "print(\"Đã lưu file data_backup.zip vào Google Drive thành công!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trong cell Colab\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip thư mục runs\n",
        "shutil.make_archive('/content/runs_backup', 'zip', '/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/runs')\n",
        "\n",
        "# Download\n",
        "files.download('/content/runs_backup.zip')"
      ],
      "metadata": {
        "id": "GN6HPIYKLvGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Zip runs\n",
        "shutil.make_archive('/content/runs', 'zip', '/content/BTL-NLP-2526I_INT3406_3/vlsp-mt')\n",
        "\n",
        "# Copy sang Drive\n",
        "shutil.copy('/content/runs.zip', '/content/drive/MyDrive/code50.zip')\n",
        "\n",
        "print(\"Done! Vào Google Drive tải về.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8DBbQCgy_6T",
        "outputId": "c15272d2-7439-42d4-d343-6b312824f648"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Done! Vào Google Drive tải về.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/clean/test.en \\\n",
        "    --output outputs/test_en2vi.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsE8V9jiMuCO",
        "outputId": "dc5ce367-172e-4914-ad8f-1eb466bbcf67"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 18:33:40.397806: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 18:33:40.416245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766428420.438876  111891 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766428420.445623  111891 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766428420.463156  111891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766428420.463204  111891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766428420.463209  111891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766428420.463213  111891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 18:33:40.468305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model: Qwen/Qwen2.5-3B-Instruct\n",
            "Loading adapter: runs/en2vi_v1/lora_en2vi_sft\n",
            "Loaded tokenizer from adapter (may include medical vocab)\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.06it/s]\n",
            "Resizing embeddings: 151936 -> 151761\n",
            "Model loaded on cuda:0, dtype=torch.bfloat16\n",
            "Loaded 1000 sentences from data/clean/test.en\n",
            "Generating:   0% 0/63 [00:00<?, ?it/s]/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/generate.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating: 100% 63/63 [12:05<00:00, 11.52s/it]\n",
            "\n",
            "Saved 1000 translations to outputs/test_en2vi.hyp.vi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/test_en2vi.hyp.vi \\\n",
        "    --ref data/clean/test.vi \\\n",
        "    --src data/clean/test.en \\\n",
        "    --show_worst 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TdjzoPRxQd",
        "outputId": "73b65dfe-c544-4fbd-9d5b-f56dee833f89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hypothesis: outputs/test_en2vi.hyp.vi\n",
            "Loading reference: data/clean/test.vi\n",
            "Loading source: data/clean/test.en\n",
            "\n",
            "Evaluating 1000 sentence pairs...\n",
            "Downloading NLTK resource: wordnet\n",
            "Downloading NLTK resource: omw-1.4\n",
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS\n",
            "==================================================\n",
            "  BLEU:    50.76\n",
            "  chrF++:  64.92\n",
            "  chrF:    64.99\n",
            "  TER:     41.70 (lower is better)\n",
            "  METEOR:  74.17\n",
            "==================================================\n",
            "\n",
            "10 WORST TRANSLATIONS (by BLEU):\n",
            "--------------------------------------------------\n",
            "\n",
            "[1] Line 499 | BLEU: 1.8 | chrF: 3.2\n",
            "  SRC: KNOWLEGE OF DISEASES IN SCHOOL, SOCIAL VICES AND BAD HOBITS OF 10TH GRADE STUDENTS IN nAMdINH PROVINCE IN 2013\n",
            "  HYP: Kiến thức về bệnh tật học đường, tệ nạn xã hội và thói quen xấu của học sinh lớp 10 tại tỉnh Nam Định năm 2013\n",
            "  REF: NHẬN THỨC VỀ MỘT SỐ BỆNH TẬT HỌC ĐƯỜNG, TỆ NẠN XÃ HỘI, THÓI QUEN XẤU CỦA HỌC SINH LỚP 10 TẠI NAM ĐỊNH NĂM 2013\n",
            "\n",
            "[2] Line 860 | BLEU: 1.9 | chrF: 1.4\n",
            "  SRC: RESULT OF TREATMENT OF SEVERE PNEUMONIA IN CHILDREN FROM 1 MONTH TO FIVE YEARS OLD IN QUANG NINH PROVINC\n",
            "  HYP: KẾT QUẢ ĐIỀU TRỊ VIÊM PHỔI NẶNG Ở TRẺ EM TỪ 1 THÁNG ĐẾN 5 TUỔI TẠI TỈNH QUẢNG NINH\n",
            "  REF: Kết quả điều trị viêm phổi nặng ở trẻ từ 1tháng đến 5 tuổi tại Quảng Ninh\n",
            "\n",
            "[3] Line 315 | BLEU: 2.8 | chrF: 5.1\n",
            "  SRC: EVALUATION OF PRELIMINARY RESULTS OF ENDOSCOPIC LITHOTRIPSY THROUGH T-TUBE TRACT AT CAN THO CENTRAL GENERAL HOSPITAL FROM 2020 TO 2021\n",
            "  HYP: Đánh giá kết quả bước đầu nội soi tán sỏi qua đường hầm Kehr điều trị sỏi đường mật chính tại Bệnh viện Đa khoa Trung ương Cần Thơ năm 2020 - 2021\n",
            "  REF: NGHIÊN CỨU ĐẶC ĐIỂM LÂM SÀNG, CẬN LÂM SÀNG VÀ ĐÁNH GIÁ KẾT QUẢ ĐIỀU TRỊ SÓT SỎI ĐƯỜNG MẬT BẰNG PHƯƠNG PHÁP TÁN SỎI ĐIỆN THUỶ LỰC QUA ĐƯỜNG HẦM KEHR TẠI BỆNH VIỆN ĐA KHOA TRUNG ƯƠNG CẦN THƠ NĂM 2020-2021\n",
            "\n",
            "[4] Line 402 | BLEU: 2.9 | chrF: 9.9\n",
            "  SRC: He hasn't had anyfurther seizure episodes.\n",
            "  HYP: Không có cơn động kinh nào xảy ra sau đó.\n",
            "  REF: Trẻ không xuất hiện thêm co giật, huyết áp được kiểm soát.\n",
            "\n",
            "[5] Line 596 | BLEU: 3.1 | chrF: 17.9\n",
            "  SRC: Minimally invasive aortic valve surgery\n",
            "  HYP: Phẫu thuật thay van động mạch chủ ít xâm lấn\n",
            "  REF: Vai trò phẫu thuật glenn trong điều trị bệnh tim bẩm sinh tím\n",
            "\n",
            "[6] Line 868 | BLEU: 3.9 | chrF: 28.5\n",
            "  SRC: Forms for and information about reporting ADRs are available in the Physicians' Desk Reference and the FDA News Daily Drug Bulletin, as well as at the FDA Adverse Event Reporting System (FAERS; MedWatch: The FDA Safety Information and Adverse Event Reporting Program).\n",
            "  HYP: Các hình thức và thông tin về báo cáo ADR có sẵn trong Physicians 'Desk Reference và FDA News Daily Drug Bulletin, cũng như tại FDA Adverse Event Reporting\n",
            "  REF: Các biểu mẫu và thông tin dùng để báo cáo các phản ứng bất lợi của thuốc có tại Physicians ’ Desk Reference (Tài liệu tham khảo tại bàn của bác sĩ) và FDA News Daily Drug Bulletin (Bản tin dược phẩm hàng ngày của FDA News), cũng như tại FDA Adverse Event Reporting System (Hệ thống báo cáo tác dụng phụ của FDA) (FAERS; MedWatch: The FDA Safety Information and Adverse Event Reporting Program (Chương trình báo cáo biến cố bất lợi và thông tin an toàn của FDA)).\n",
            "\n",
            "[7] Line 432 | BLEU: 4.1 | chrF: 31.9\n",
            "  SRC: We suggest that FRS should be used to estimate cardiovascular disease risk for psoriasis patients and early management of these risk is required to prevent cardiovascular diseases.\n",
            "  HYP: Chúng tôi khuyến cáo nên sử dụng thang điểm FRS để đánh giá nguy cơ bệnh lý dạ dày - ruột cho bệnh nhân vảy nến và cần quản lý sớm những bệnh nhân có nguy cơ cao để phòng ngừa bệnh lý dạ dày - ruột.\n",
            "  REF: Chúng tôi đề nghiự́ng dụng FRS để tiên lượng nguy cơ mắc bệnh tim mạch của bệnh nhân vẩy nến và đồng thời có biện pháp can thiệp sớm các nguy cơ nhằm làm giảm nguy cơ này.\n",
            "\n",
            "[8] Line 662 | BLEU: 4.4 | chrF: 22.3\n",
            "  SRC: CONCLUSION: Our experience of choosing the iliac fossa for kidney transplantation has showed that choosing RIF for all cases with RKD or LKD reveal no complication.\n",
            "  HYP: KẾT LUẬN: Kinh nghiệm của chúng tôi về việc lựa chọn hố chậu để ghép cho tất cả các trường hợp có RKD hay LKD đều cho thấy không có biến chứng nào xảy ra.\n",
            "  REF: Bàn luận và kết luận: Kinh nghiệm về việc chọn lựa đường mổ chủ yếu là bên P trong ghép thận của loạt nghiên cứu: cho thấy có thể tiến hành chọn ghép bên HCP cho mọi quả thận lấy bằng phương pháp mổ mở hay mổ nội soi.\n",
            "\n",
            "[9] Line 447 | BLEU: 5.2 | chrF: 25.8\n",
            "  SRC: Occasionally, when cystic, they are soft but most often they are firm.\n",
            "  HYP: Đôi khi, khi nang hoá, chúng mềm nhưng thường là cứng.\n",
            "  REF: Đôi khi khôi u dạng nang, mềm, nhưng đa số trường hợp u có mật độ chắc.\n",
            "\n",
            "[10] Line 101 | BLEU: 5.4 | chrF: 33.0\n",
            "  SRC: At the beginning of each period, a thorough professional prophylaxis was performed to remove all the dental plaque, stain, debris, calculus.\n",
            "  HYP: Tại thời điểm bắt đầu mỗi đợt điều trị, tiến hành làm sạch toàn bộ mảng bám, vết ố, mảnh vụn, cao răng.\n",
            "  REF: Vôi răng, mảng bám và vết dính được làm sạch hoàn toàn vào đầu mỗi giai đoạn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate.py \\\n",
        "    --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --adapter_path runs/en2vi_v1/lora_en2vi_sft \\\n",
        "    --direction en2vi \\\n",
        "    --input data/raw/test_unseen_v3.en.txt \\\n",
        "    --output outputs/test_unseen_v3.hyp.vi \\\n",
        "    --batch_size 16 \\\n",
        "    --num_beams 4 \\\n",
        "    --repetition_penalty 1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGIuHPdET2bE",
        "outputId": "8cd358f5-43fa-48a3-8cfb-2155b54c9eb7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 20:54:06.954644: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-22 20:54:06.972270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766436846.994049  147099 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766436847.000536  147099 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766436847.017108  147099 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766436847.017147  147099 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766436847.017150  147099 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766436847.017153  147099 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 20:54:07.021995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model: Qwen/Qwen2.5-3B-Instruct\n",
            "Loading adapter: runs/en2vi_v1/lora_en2vi_sft\n",
            "Loaded tokenizer from adapter (may include medical vocab)\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.07it/s]\n",
            "Resizing embeddings: 151936 -> 151761\n",
            "Model loaded on cuda:0, dtype=torch.bfloat16\n",
            "Loaded 1152 sentences from data/raw/test_unseen_v3.en.txt\n",
            "Generating:   0% 0/72 [00:00<?, ?it/s]/content/BTL-NLP-2526I_INT3406_3/vlsp-mt/scripts/generate.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating: 100% 72/72 [13:52<00:00, 11.56s/it]\n",
            "\n",
            "Saved 1152 translations to outputs/test_unseen_v3.hyp.vi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data/raw/test_unseen_v3.en.txt"
      ],
      "metadata": {
        "id": "-wv59csS1dnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/eval_bleu.py \\\n",
        "    --hyp outputs/test_unseen_v3.hyp.vi \\\n",
        "    --ref data/raw/test_unseen_v3.vi.txt \\\n",
        "    --src data/raw/test_unseen_v3.en.txt \\\n",
        "    --show_worst 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZQC6H2N1aCz",
        "outputId": "4d5f8e56-4c34-4f48-8f17-e26cca76aec2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hypothesis: outputs/test_unseen_v3.hyp.vi\n",
            "Loading reference: data/raw/test_unseen_v3.vi.txt\n",
            "Loading source: data/raw/test_unseen_v3.en.txt\n",
            "\n",
            "Evaluating 1152 sentence pairs...\n",
            "Downloading NLTK resource: wordnet\n",
            "Downloading NLTK resource: omw-1.4\n",
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS\n",
            "==================================================\n",
            "  BLEU:    50.15\n",
            "  chrF++:  64.16\n",
            "  chrF:    64.33\n",
            "  TER:     43.38 (lower is better)\n",
            "  METEOR:  73.20\n",
            "==================================================\n",
            "\n",
            "10 WORST TRANSLATIONS (by BLEU):\n",
            "--------------------------------------------------\n",
            "\n",
            "[1] Line 492 | BLEU: 2.8 | chrF: 7.8\n",
            "  SRC: Pherochromocytomas in children - Genotypic and phenotype\n",
            "  HYP: Pherochromocytomas in children - Genotypic and phenotype\n",
            "  REF: U tuỷ thượng thận ở trẻ em - Kiểu gen và kiểu hình\n",
            "\n",
            "[2] Line 847 | BLEU: 4.1 | chrF: 27.7\n",
            "  SRC: Hemorrhagic strokes consist of a highest rate 86, 7% (26/30 patients), transient ischemic attack rate is 10% (3/30 patients), neurological deficits rate is 3, 3% (1/30 patients) Conclusions: Moyamoya disease is a chronic cerebrovascular occlusive disease with a long history carrying risk of either ischemic or haemorrhage events. Revascularization surgery in patients with Moyamoya disease and cerebrovascular occlusive disease carries a low risk, is effective at preventing future ischemic events, as well as furture strokes and improves quality of life.\n",
            "  HYP: Kết luận: Phẫu thuật can thiệp nội mạch trên bệnh nhân Moyamoya và bệnh lý tắc nghẽn mạch máu có nguy cơ cao xảy ra biến cố thiếu máu cục bộ cũng như đột quỵ, phẫu thuật can thiệp nội mạch trên bệnh nhân Moyamoya và bệnh lý tắc nghẽn mạch máu có nguy cơ cao xảy ra biến cố thiếu máu cục bộ cũng như đột quỵ, phẫu thuật can thiệp nội mạch trên bệnh nhân Moyamoya và bệnh lý tắc nghẽn mạch máu có nguy cơ cao xảy ra biến cố thiếu máu cục bộ cũng như\n",
            "  REF: Bệnh nhân biểu hiện đột quị chiếm tỉ lệ cao nhất 86,7% (26/3 0), kế đến là triệu chứng cơn thoáng thiếu máu não chiếm tỉ lệ 10% (3/3 0), dấu thần kinh khu trú chiếm tỉ lệ 3,3% (1/3 0), không ghi nhận triệu chứng động kinh trong nghiên cứu này Kết luận: Bệnh moyamoya là bệnh lý tắc nghẽn mãn tính động mạch não với bệnh sử kéo dài và có nguy cơ thiếu máu não hoặc xuất huyết não.Phẫu thuật bắc cầu động mạch não ở bệnh lý moyamoya, với tỉ lệ tai biến thấp, có hiệu quả trong việc ngăn chặn nguy cơ nhồi máu não tái phát, cũng như giảm nguy cơ xuất huyết não và cải thiện chất lượng cuộc sống.\n",
            "\n",
            "[3] Line 1104 | BLEU: 4.4 | chrF: 22.1\n",
            "  SRC: RESEARCH ON CRICULATION SUPPORT EFFECTS OF VENO ARTERIAL EXTRACORPOREAL MEMBRANE OXYGENATION (VA-ECMO) IN TREATMENTING ACUTE MYOCARDITIS PATIENTS\n",
            "  HYP: NGHIÊN CỨU HIỆU QUẢ HỖ TRỢ TẾ BÀO CƠ TIM CỦA KỸ THUẬT OXY HOÁ MÁU TÓM TẮT Nghiên cứu hiệu quả hỗ trợ tế bào cơ tim của kỹ thuật oxy hoá máu qua màng ngoài cơ thể (VA-ECMO) trong điều trị bệnh nhân viêm cơ tim cấp\n",
            "  REF: NGHIÊN CỨU KẾT QUẢ HỖ TRỢ TUẦN HOÀN CỦA PHƯƠNG PHÁP TRAO ĐỔI OXY QUA MÀNG NGOÀI CƠ THỂ (ECMO) TRONG ĐIỀU TRỊ BỆNH NHÂN VIÊM CƠ TIM CẤP\n",
            "\n",
            "[4] Line 694 | BLEU: 5.2 | chrF: 41.5\n",
            "  SRC: The genotypes observed were in agreement with those expected under Hardy-Weinberg equilibrium.\n",
            "  HYP: Các kiểu gen thu được phù hợp với cân bằng Hardy-Weinberg.\n",
            "  REF: Sự phân bố kiểu gen ở quần thể nghiên cứu tuân theo quy luật cân bằng Hardy - Weinberg.\n",
            "\n",
            "[5] Line 556 | BLEU: 6.0 | chrF: 35.1\n",
            "  SRC: In this trial, adding simethicone to bowel preparation improved the effectiveness of bowel preparation.\n",
            "  HYP: Trong thử nghiệm này, việc bổ sung simethicone vào quá trình chuẩn bị ruột đã cải thiện hiệu quả chuẩn bị ruột.\n",
            "  REF: Trong nghiên cứu này, kết hợp simethicone trong chuẩn bị NSĐT tăng hiệu quả làm sạch bọt ở đại tràng.\n",
            "\n",
            "[6] Line 1046 | BLEU: 6.1 | chrF: 21.6\n",
            "  SRC: There were 20.3% patients hadn't wore protected hat, in cases had wore protected hat there were 26.2% the protected hat out of the head after accidents.\n",
            "  HYP: Có 20,3% bệnh nhân không đội mũ bảo vệ, trong số có đội mũ bảo vệ có 26,2% mũ bảo vệ bị tuột ra sau tai nạn.\n",
            "  REF: Tỷ lệ các trường hợp không đội nón bảo hiểm còn cao (23,2%), trong số những hợp trường hợp có đội nón thì tỷ lệ nón rơi ra khỏi đầu cao (26,2%).\n",
            "\n",
            "[7] Line 665 | BLEU: 6.1 | chrF: 35.2\n",
            "  SRC: Good recovery with no disability accounted for 87.21% of all patients during the 3 - month follow up.\n",
            "  HYP: Kết quả phục hồi tốt không tàn tật chiếm 87,21% tổng số bệnh nhân trong thời gian theo dõi 3 tháng.\n",
            "  REF: 87,21% bệnh nhân không có di chứng sau 3 tháng xuất viện.\n",
            "\n",
            "[8] Line 267 | BLEU: 6.2 | chrF: 18.8\n",
            "  SRC: Less may become cancer cell.\n",
            "  HYP: Ít có thể trở thành tế bào ung thư.\n",
            "  REF: Một tỷ lệ nhỏ những bệnh nhân V.A quá phát này có thể chuyển sản thành mô ung thư.\n",
            "\n",
            "[9] Line 683 | BLEU: 6.3 | chrF: 29.4\n",
            "  SRC: The study was conducted from 01/01/2015 to 30/09/2019 of 98 victims who died of brain damage.\n",
            "  HYP: Nghiên cứu được tiến hành từ 01/01/2015 đến 30/09/2019 của 98 nạn nhân tử vong do tổn thương ống ống sống.\n",
            "  REF: Nghiên cứu được thực hiện từ ngày 01 tháng 01 năm 2015 đến ngày 30 tháng 9 năm 2019 chúng tôi thu thập được 98 trường hợp nạn nhân là những người bị chết có tổn thương dập não.\n",
            "\n",
            "[10] Line 1044 | BLEU: 6.6 | chrF: 25.8\n",
            "  SRC: The level of brain injury of patents when this resolution had done.\n",
            "  HYP: Mức độ tổn thương của bệnh nhân khi được thực hiện thủ thuật này.\n",
            "  REF: Mức độ nghiêm trọng của chấn thương sọ não sau Nghị định.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCW51sXR1yU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}